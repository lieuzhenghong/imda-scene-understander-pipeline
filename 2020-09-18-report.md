---
title: Daily report
date: 18th September 2020
---

## Notes from engineer startup meeting

- They are working on Free Fleet -> drop in replacement with fleet manager

- we don't have fleet adapter how to do

For fleet director, we are saying
"handling of SUM input to change route --> you need to code the logic to change the possible routes"

The problem is that OK, you can detect the wheelchair at any possible points
Hardcoding the routes?? Why are we doing this?

RMF traffic gives the possible routes and requests from the individual fleet adapter
and hence

Fleet director is something than Oliver and Eric has been proposing???

Why do we need to rewrite the fleet director? Why cannot use RMF traffic???

Conflict management (resolving conflicts) -- using SVM? what is SVM?

My first order of business is to figure out how to send a stream of JSON files from
the robots (running Python) to the server (running RMF Core and SOSS)

But I don't want to duplicate any work that the Engineering team has already done

RMF app has not been written yet????

Look at the following links:

[](https://osrf.github.io/ros2multirobotbook/soss.html)
[](https://soss.docs.eprosima.com/en/latest/index.html)

Take the output of scene understanding to do some optical avoidance

There is a fleet director in VAMA 1.0 that directs the robots around ---> basically doing what RMF core is supposed to do

The Fleet Director is something that takes in the SU output and makes a decision as to how to

All the previous code was written in Java Swing --> now need to tear down and rewrite to be compatible with RMF core

## Projects

My understanding after speaking to Eric yesterday that there are two separate
scene understanding models.

The first model is an object detection model called the Scene Understanding Module. It lives on the robot,
and it does the following:

1. Takes in RTSP stream from its webcam
2. Converts the stream into images
3. Creates a list of bounding boxes List[BoundingBox], which is then saved as a JSON file

The on-device ML model's function signature is `RTSP -> Image -> List[Bounding Box] JSON`.

On the cloud there is a machine running RMF.
This machine also runs a model called the **Scene Understanding Manager**
that receives the different bounding boxes from the different robots
and constructs a List of bounding polygons.

The way it receives the bounding boxes from the different robots should be
through the ROS2 pub-sub system.

`List[Bounding Box] JSON -> List[Bounding Poly] JSON`

The list of bounding polygon JSON is then passed to the RMF dispatcher
(**EDIT**: but not clear on this, there is a Fleet Director in the middle,
this has to be dealt with by the engineering team)
which will then decide whether to reroute the robots or not.

The list of tasks that I believe are important:

0. In order to do any sort of integration or plumbing,
   I need to understand RMF, ROS2, and SOSS very well first.
1. Find a way to publish the List[Bounding Box] generated by the robot
   to the RMF core server (most probably using ROS2 or SOSS).
2. Yifan mentioned that it would be very helpful for him to understand
   how the SUM modules work.
   So I'm going to create a diagram for Yifan that explains the relationship
   between the two Scene Understanding Modules.
   Clear it with Eric first.

3. [First Model] Use DeepStream to get RTSP stream into images, or if possible, do end-to-end
   learning
4. Downstream integration: need to know how to pass the 3D bounding box to whatever
   the relevant RMF Core component is
5. (In the future) We can package the models in a nice way and share them with
   the public; maybe I can help with deployment.

## Update after reading

The [ROS 2 wiki](https://index.ros.org/doc/ros2/Concepts/#conceptshome) is very helpful.
I now have a better understanding of what needs to be done.

ROS supports backpressure (see link [here](https://index.ros.org/doc/ros2/Tutorials/Writing-A-Simple-Py-Publisher-And-Subscriber/)):

> `create_publisher` declares that the node publishes messages of type String (imported from the std_msgs.msg module), over a topic named topic, and that the “queue size” is 10. Queue size is a required QoS (quality of service) setting that limits the amount of queued messages if a subscriber is not receiving them fast enough.

## What do I need to do ?

1. Set up `rmf_core` either on my development machine or on IMDA's server
2. Set up schemas for bounding box JSON (create a ROS2 `.msg` file)
3. Set up a "robot" and write the publisher code in Python that publishes bounding box messages
4. Set up a ROS2 subscriber in C++ on the RMF server and
   write the listener code to receive bounding box messages
5. [Needs confirmation] Pass the list of bounding boxes to the
   Scene Understanding Manager ML model

Refer to `demo_nodes_py` and `demo_nodes_cpp` talker and listeners.

### Do I need SOSS?

From the [OSRF documentation](https://osrf.github.io/ros2multirobotbook/intro.html):

**Systems of Systems Synthesizer (SOSS)**

The SOSS you've been missing! SOSS easily passes messages between various message formats and types, including ROS 1, ROS 2, WebSocket, REST, FiWare, DDS, OPC-UA, and more.

Ask James --- is there already a pipeline?
