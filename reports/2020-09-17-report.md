---
title: Daily report
date: 17th September 2020
---

# What I'm doing today

1. Learn about how GStreamer works (read the tutorials)
2. Get GStreamer installed and working on my computer
3. Write a function that does what I want in C
4. learn how the C++ bindings work
5. Convert that C function to C++ using the bindings

## Installing GStreamer on Ubuntu or Debian

```bash
apt-get install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio
```

## Compiling the code

```bash
gcc basic-tutorial-1.c -o basic-tutorial-1 `pkg-config --cflags --libs gstreamer-1.0`
```

> **Warning**: Depending on the GStreamer libraries you need to use,
> you will have to add more packages to the pkg-config command,
> besides gstreamer-1.0 At the bottom of each tutorial's source code
> you will find the command for that specific tutorial, including
> the required libraries, in the required order. When developing
> your own applications, the GStreamer documentation will tell you
> what library a function belongs to.

## Understanding the GStreamer pipeline

### The GStreamer Bus

At this point it is worth introducing the GStreamer bus a bit more
formally. It is the object responsible for delivering to the
application the GstMessages generated by the elements, in order and
to the application thread. This last point is important, because tho
actual streaming of media is done in another thread than the
application.

Messages can be extracted from the bus synchronously with
gst_bus_timed_pop_filtered() and its siblings, or asynchronously,
using signals (shown in the next tutorial). Your application should
always keep an eye on the bus to be notified of errors and other
playback-related issues.

The rest of the code is the cleanup sequence, which is the same as
in Basic tutorial 1: Hello world!.

## Understanding the tutorials

## Got `gst_parse_launch` working

I managed to get the [SO link](https://stackoverflow.com/questions/59025321/capture-jpeg-images-from-rtsp-gstreamer) I saw yesterday
working correctly.

Then I spent about two hours trying to figure out how to write the images
to an array/vector/buffer instead of to file. It was really difficult because
it was all in C.

But after struggling with this for a long time, I asked myself: why am I doing this?
If the scene

## Spoke to Eric and realised that I was on the wrong path all the while

**I need to understand the RMF system.**

After speaking with Eric, I understood that in fact there is no need for any
GStreamer at all because the only thing that consumes video streams is in fact
Python code! And the functionality that takes video streams and returns images
is in fact most probably already easily done with Nvidia's DeepStream.

The misunderstanding stems from the fact that Eric believed that we needed GStreamer
to pass a stream of JSON objects to the C++ RMF library. In reality, I told him
that just some sort of HTTP API will be needed ([backpressure](https://medium.com/@jayphelps/backpressure-explained-the-flow-of-data-through-software-2350b3e77ce7) notwithstanding).

### SU Module (lives on the robot)

The Scene Understanding Module (SUM) is a Python module on the robot that does the following:

1. Takes in RTSP stream from its webcam
2. Converts the stream into images
3. Creates a JSON file

The on-device ML model's function signature is `RTSP -> Image -> 2D Bounding Box JSON`.

On the cloud there is a machine running RMF as well as some sort of server
that can receive the 2D Bounding Box JSON files from the robots.
It then runs some sort of model that converts this 2D bounding box JSON
into a 3D scene, drawing 3D bounding polygons.

This 3D bounding polygon JSON is then passed to the RMF dispatcher,
which will then decide whether to reroute the robots or not.

### How do robots and other devices (lifts etc.) communicate with RMF core?

What is the web transmission protocol?
RMF sits on a server. How do the robots talk with RMF??

I wrote a GitHub issue [here](https://github.com/osrf/rmf_core/issues/170)
and Grey was kind enough to reply me promptly.

[WebSocket Streams API](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API)

We actually don't need anything like this. We only need one directional communication:
a HTTPS POST request is enough.

#### How do lifts communicate with the RMF core server?

Eric says there's an API via the gateway and I'll join the standup call with the
lift engineering team at 0845 tomorrow.

#### Robots communicate with one another over ROS

Link [here](https://index.ros.org/doc/ros2/Concepts/#id4)

> Discovery of nodes happens automatically through the underlying middleware
> of ROS 2. It can be summarized as follows:

1. When a node is started, it advertises its presence to other nodes on the
   network with the same ROS domain (set with the ROS_DOMAIN_ID environment
   variable). Nodes respond to this advertisement with information about
   themselves so that the appropriate connections can be made and the nodes can
   communicate.

2. Nodes periodically advertise their presence so that connections can be made with new-found entities, even after the initial discovery period.

3. Nodes advertise to other nodes when they go offline.

> Nodes will only establish connections with other nodes if they have compatible Quality of Service settings.

> Take the talker-listener demo for example. Running the C++ talker node in one terminal will publish messages on a topic, and the Python listener node running in another terminal will subscribe to messages on the same topic.

> You should see that these nodes discover each other automatically, and
> begin to exchange messages.
